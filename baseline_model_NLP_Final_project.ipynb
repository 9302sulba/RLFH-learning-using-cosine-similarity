{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# === Load Word List from Excel ===\n",
    "excel_path = \"/content/Words.xlsx\"\n",
    "df_words = pd.read_excel(excel_path)\n",
    "\n",
    "word_list = [str(w) for w in df_words['Word'].dropna() if str(w).isalpha()]\n",
    "\n",
    "# === Load SmolLM2-135M ===\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_definition(word):\n",
    "    prompt = f\"Define {word}.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def get_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "# === Generate Baseline Definitions and Embeddings ===\n",
    "results = []\n",
    "for word in word_list:\n",
    "    try:\n",
    "        definition = get_definition(word)\n",
    "        embedding = get_embedding(word)\n",
    "        results.append({\n",
    "            \"word\": word,\n",
    "            \"definition\": definition,\n",
    "            \"embedding\": embedding.tolist()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for word '{word}': {e}\")\n",
    "\n",
    "# === Save Results ===\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_pickle(\"baseline_definitions_embeddings.pkl\")\n",
    "print(\"Baseline results saved to baseline_definitions_embeddings.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "df = pd.read_pickle(\"baseline_definitions_embeddings.pkl\")\n",
    "\n",
    "# View first few rows\n",
    "df.head(10) # or use display(df.head()) in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#  WordNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Baseline Results\n",
    "df_results = pd.read_pickle(\"/content/baseline_definitions_embeddings.pkl\")\n",
    "\n",
    "#  WordNet Definition\n",
    "def get_wordnet_definition(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#  Adding WordNet Definitions\n",
    "df_results[\"wordnet_definition\"] = df_results[\"word\"].apply(get_wordnet_definition)\n",
    "df_results.to_pickle(\"results_with_wordnet.pkl\")\n",
    "print(\"WordNet definitions added and saved to results_with_wordnet.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "df = pd.read_pickle(\"results_with_wordnet.pkl\")\n",
    "\n",
    "# View first few rows\n",
    "df.head(10) # or use display(df.head()) in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"results_with_wordnet.pkl\")\n",
    "\n",
    "def clean_definition(row):\n",
    "    word = row[\"word\"]\n",
    "    text = row[\"definition\"]\n",
    "    return text.replace(f\"Define {word}.\", \"\").strip()\n",
    "df[\"definition_clean\"] = df.apply(clean_definition, axis=1)\n",
    "\n",
    "df[[\"word\", \"definition\", \"definition_clean\"]].head(10)\n",
    "\n",
    "df.to_pickle(\"results_with_wordnet_cleaned.pkl\")\n",
    "print(\"Cleaned definitions saved to results_with_wordnet_cleaned.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"results_with_wordnet_cleaned.pkl\")\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"results_with_wordnet_cleaned.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"results_with_wordnet_cleaned.pkl\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "def get_definition_similarity(row):\n",
    "    def1 = row[\"definition_clean\"]\n",
    "    def2 = row[\"wordnet_definition\"]\n",
    "\n",
    "    if pd.isna(def1) or pd.isna(def2):\n",
    "        return None\n",
    "\n",
    "    emb1 = model.encode(def1)\n",
    "    emb2 = model.encode(def2)\n",
    "\n",
    "    return cosine_similarity([emb1], [emb2])[0][0]\n",
    "df[\"definition_similarity\"] = df.apply(get_definition_similarity, axis=1)\n",
    "\n",
    "df.to_pickle(\"results_with_similarity.pkl\")\n",
    "print(\"Similarity scores saved to results_with_similarity.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "df = pd.read_pickle(\"results_with_similarity.pkl\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average similarity:\", df[\"definition_similarity\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "df_baseline = pd.read_pickle(\"/content/results_with_similarity (1).pkl\")\n",
    "\n",
    "df_baseline.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean = df_baseline[\"definition_similarity\"].mean()\n",
    "print(\"Baseline mean similarity:\", baseline_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4gYvzk6HSZA"
   },
   "source": [
    "# Loading PPO finetuned file here for comparsion with baseline.\n",
    "A separate file is used for PPO training, where the model was fine-tuned to generate new definitions and corresponding embeddings. The resulting .pkl file is then used here for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the .pkl file\n",
    "df_finetune = pd.read_pickle(\"/content/finetuned_results_with_similarity.pkl\")\n",
    "\n",
    "# View first few rows\n",
    "df_finetune.head(10) # or use display(df.head()) in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune_mean = df_finetune[\"definition_similarity\"].mean()\n",
    "print(\"mean similarity finetune vs wordnet(binary Binary Reward (threshold = 0.85)):\", df_finetune_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Improvement: {df_finetune_mean - baseline_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading both dataframes\n",
    "df_base = pd.read_pickle(\"/content/results_with_similarity (1).pkl\")\n",
    "df_finetune = pd.read_pickle(\"/content/finetuned_results_with_similarity.pkl\")\n",
    "\n",
    "df_base = df_base.dropna(subset=[\"definition\", \"definition_similarity\"])\n",
    "df_finetune = df_finetune.dropna(subset=[\"definition\", \"definition_similarity\"])\n",
    "\n",
    "df_compare = pd.merge(df_base, df_finetune, on=\"word\", suffixes=(\"_baseline\", \"_finetuned\"))\n",
    "\n",
    "df_compare[\"similarity_gain\"] = df_compare[\"definition_similarity_finetuned\"] - df_compare[\"definition_similarity_baseline\"]\n",
    "\n",
    "df_compare[\"gain\"] = df_compare[\"definition_similarity_finetuned\"] - df_compare[\"definition_similarity_baseline\"]\n",
    "\n",
    "percent_improved = (df_compare[\"gain\"] > 0).mean() * 100\n",
    "\n",
    "# Print result\n",
    "print(f\"Words with improved similarity after fine-tuning: {percent_improved:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
